{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c76646",
   "metadata": {},
   "source": [
    "# NAIVE APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce5af8",
   "metadata": {},
   "source": [
    "**1. What is the Naive Approach in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f747b066",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a supervised machine learning algorithm, which is used for classification tasks, like text classification.Naive Bayes is called naive because it assumes that each input variable is independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5158c",
   "metadata": {},
   "source": [
    "**2. Explain the assumptions of feature independence in the Naive Approach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fef5c",
   "metadata": {},
   "source": [
    "Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.It assumes that each input variable is independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea777e",
   "metadata": {},
   "source": [
    "**3. How does the Naive Approach handle missing values in the data?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "419e0f6a",
   "metadata": {},
   "source": [
    "Naïve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information according to the probability estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6219b",
   "metadata": {},
   "source": [
    "**4. What are the advantages and disadvantages of the Naive Approach?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c067c096",
   "metadata": {},
   "source": [
    "Advantages of Naive Approach:\n",
    "1.This algorithm works quickly and can save a lot of time. \n",
    "2.Naive Bayes is suitable for solving multi-class prediction problems. \n",
    "3.If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data. \n",
    "4.Naive Bayes is better suited for categorical input variables than numerical variables.\n",
    "\n",
    "Disadvantages of Naive Approach:\n",
    "1.Naive Bayes assumes that all predictors (or features) are independent, rarely happening in real life. This limits the applicability of this algorithm in real-world use cases.\n",
    "2.This algorithm faces the ‘zero-frequency problem’ where it assigns zero probability to a categorical variable whose category in the test data set wasn’t available in the training dataset. It would be best if you used a smoothing technique to overcome this issue.\n",
    "3.Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e298547",
   "metadata": {},
   "source": [
    "**5. Can the Naive Approach be used for regression problems? If yes, how**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a0bae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f9a6d2",
   "metadata": {},
   "source": [
    "**6. How do you handle categorical features in the Naive Approach?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6854b6c",
   "metadata": {},
   "source": [
    "To handle categorical features in the Naive Approach is to create each category as a feature and with boolean values. Not only this way removes the limitation of categories for some of the libraries (no applicable here since you have written your own function) but also it's fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48519b1",
   "metadata": {},
   "source": [
    "**7. What is Laplace smoothing and why is it used in the Naive Approach?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4dd2f82e",
   "metadata": {},
   "source": [
    "It is introduced to solve the problem of zero probability i.e. when a query point contains a new observation, which is not yet seen in training data while calculating probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f664d35",
   "metadata": {},
   "source": [
    "**8. How do you choose the appropriate probability threshold in the Naive Approach?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec7601e4",
   "metadata": {},
   "source": [
    "The default value for the minimum threshold of probabilities is 0.001. Valid values are positive numbers that are smaller than.The Naive Bayes method can only work with discrete input fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8edb9",
   "metadata": {},
   "source": [
    "**9. Give an example scenario where the Naive Approach can be applied.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64638b3a",
   "metadata": {},
   "source": [
    "1.It is used for Credit Scoring.:- Credit evaluation is an important process in credit lending institutions. Given the increasing demand for credit facilities as well as rapid growth of financial institutions has led to the need for effective tools and techniques of handling credit evaluation procedures. These tools should also be capable of speeding up these\n",
    "processes to ensure quick and quality delivery of service to the customers in the financial sector. The institutions\n",
    "employ credit officers to make credit decisions or recommendations. The lending officer usually makes approval for\n",
    "applications that are worthy of giving a loan. These officers are given some guidelines to direct them in evaluating\n",
    "the worthiness of credit applications depending on the borrowers’ characteristics. Further, credit officers sifting\n",
    "large volumes of data concerning a customer as part of the decision making process, which might in some way be\n",
    "biased can significantly slow down the process. This practice is inefficient, inconsistent and non-uniform.\n",
    "In the recent past, researchers have incorporated Naive Bayes Classifier techniques into easy-to-use toolsets in turn\n",
    "enabling the development of decision support systems in a diverse set of application domains some of which include\n",
    "medical diagnosis, safety assessment, equipment fault diagnosis, credit analysis, software quality and procurement. Therefore,  credit evaluation model that utilizes capability of Naive Bayes Classifier as an enabling tool for decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fbb5c",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879c11f",
   "metadata": {},
   "source": [
    "**10. What is the K-Nearest Neighbors (KNN) algorithm?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cd30957",
   "metadata": {},
   "source": [
    "KNN classifier is a machine learning algorithm used for classification and regression problems. It works by finding the K nearest points in the training dataset and uses their class to predict the class or value of a new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049749e0",
   "metadata": {},
   "source": [
    "**11. How does the KNN algorithm work?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d76ad23",
   "metadata": {},
   "source": [
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054d800",
   "metadata": {},
   "source": [
    "**12. How do you choose the value of K in KNN?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76817719",
   "metadata": {},
   "source": [
    "The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27801d8",
   "metadata": {},
   "source": [
    "**13. What are the advantages and disadvantages of the KNN algorithm?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30bbbbf5",
   "metadata": {},
   "source": [
    "Advantages of the KNN algorithm:-\n",
    "\n",
    "1.No Training Period-- KNN modeling does not include training period as the data itself is a model which will be the reference for future prediction and because of this it is very time efficient in term of improvising for a random modeling on the available data.\n",
    "2.Easy Implementation-- KNN is very easy to implement as the only thing to be calculated is the distance between different points on the basis of data of different features and this distance can easily be calculated using distance formula such as- Euclidian or Manhattan.\n",
    "3.As there is no training period thus new data can be added at any time since it wont affect the model.\n",
    "\n",
    "\n",
    "Disadvantages of the KNN algorithm:-\n",
    "\n",
    "1.Does not work well with large dataset as calculating distances between each data instance would be very costly.\n",
    "2.Does not work well with high dimensionality as this will complicate the distance calculating process to calculate distance for each dimension.\n",
    "3.Sensitive to noisy and missing data\n",
    "4.Feature Scaling- Data in all the dimension should be scaled (normalized and standardized) properly .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5de071",
   "metadata": {},
   "source": [
    "**14. How does the choice of distance metric affect the performance of KNN?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d39b1306",
   "metadata": {},
   "source": [
    "The experimental results show that the performance of KNN classifier depends significantly on the distance used, and the results showed large gaps between the performances of different distances. We found that a recently proposed nonconvex distance performed the best when applied on most data sets comparing with the other tested distances. In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing with other distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04bd379",
   "metadata": {},
   "source": [
    "**15. Can KNN handle imbalanced datasets? If yes, how?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63343aca",
   "metadata": {},
   "source": [
    "In imbalanced datasets, kNN becomes biased towards the majority instances of the training space. To solve this problem, we propose a method called Proximity weighted Evidential kNN classifier.In this method, each neighbor of a query instance is considered as a piece of evidence from which we calculate the probability of class label given feature values to provide more preference to the minority instances. This is then discounted by the proximity of the neighbor to prioritize the closer instances in the local neighborhood. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505d6dd",
   "metadata": {},
   "source": [
    "**16. How do you handle categorical features in KNN?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb830414",
   "metadata": {},
   "source": [
    "In KNN, categorical data can be handled by converting them into numerical values using techniques such as one-hot encoding or label encoding. One-hot encoding creates a binary vector for each category, where each vector has a length equal to the number of unique categories in the feature. Label encoding assigns a unique integer value to each category. Once the categorical data has been converted into numerical values, the KNN algorithm can be applied as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6feb44",
   "metadata": {},
   "source": [
    "**17. What are some techniques for improving the efficiency of KNN?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10bf8431",
   "metadata": {},
   "source": [
    "1.Choose the right k\n",
    "2.Reduce the dimensionality\n",
    "3.Use an index structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed725670",
   "metadata": {},
   "source": [
    "**18. Give an example scenario where KNN can be applied.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e88e818",
   "metadata": {},
   "source": [
    "With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party 'Congress', “Will Vote to Party 'BJP'. Other areas in which KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660366d1",
   "metadata": {},
   "source": [
    "# Clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326bf05",
   "metadata": {},
   "source": [
    "**19. What is clustering in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70ce785f",
   "metadata": {},
   "source": [
    "Clustering is the act of organizing similar objects into groups within a machine learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9145380",
   "metadata": {},
   "source": [
    "\n",
    "**20. Explain the difference between hierarchical clustering and k-means clustering.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3cc723e",
   "metadata": {},
   "source": [
    "K Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.\n",
    "Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce785a",
   "metadata": {},
   "source": [
    "**21. How do you determine the optimal number of clusters in k-means clustering?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad00ddc8",
   "metadata": {},
   "source": [
    "There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08d2ad",
   "metadata": {},
   "source": [
    "**22. What are some common distance metrics used in clustering?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e4f2623",
   "metadata": {},
   "source": [
    "The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff603226",
   "metadata": {},
   "source": [
    "**23. How do you handle categorical features in clustering?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da7c60ee",
   "metadata": {},
   "source": [
    "One-Hot Encoding. One way to handle categorical variables is to use one-hot encoding. ...\n",
    "Mixed Clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d75a7",
   "metadata": {},
   "source": [
    "**24. What are the advantages and disadvantages of hierarchical clustering?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c36031a0",
   "metadata": {},
   "source": [
    "Advantage of hierarchical clustering : \n",
    "Clear Line of Authority. \n",
    "Clear Lines of Communication. \n",
    "Clear Results. \n",
    "\n",
    "\n",
    "\n",
    "Disadvantage of hierarchical clustering : \n",
    "Isolation and Siloed Thinking. \n",
    "Centralization of Power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb146595",
   "metadata": {},
   "source": [
    "**25. Explain the concept of silhouette score and its interpretation in clustering.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78006968",
   "metadata": {},
   "source": [
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341a31e",
   "metadata": {},
   "source": [
    "**26. Give an example scenario where clustering can be applied.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b1f51cc",
   "metadata": {},
   "source": [
    "Business Applications of Clustering : Customers are categorized by using clustering algorithms according to their purchasing behavior or interests to develop focused marketing campaigns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee1205",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956acb40",
   "metadata": {},
   "source": [
    "**27. What is anomaly detection in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dc0c8af",
   "metadata": {},
   "source": [
    "Anomaly detection is a process of finding those rare items, data points, events, or observations that make suspicions by being different from the rest data points or observations. Anomaly detection is also known as outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083adc6",
   "metadata": {},
   "source": [
    "**28. Explain the difference between supervised and unsupervised anomaly detection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c1f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f5df8e5",
   "metadata": {},
   "source": [
    "**29. What are some common techniques used for anomaly detection?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3283cfb3",
   "metadata": {},
   "source": [
    "Some of the popular techniques are:\n",
    "1.Statistical (Z-score, Tukey's range test and Grubbs's test)\n",
    "2.Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\n",
    "3.Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc251b3d",
   "metadata": {},
   "source": [
    "**30. How does the One-Class SVM algorithm work for anomaly detection?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "312f41fd",
   "metadata": {},
   "source": [
    "One-Class Support Vector Machine (SVM) is an unsupervised model for anomaly or outlier detection. Unlike the regular supervised SVM, the one-class SVM does not have target labels for the model training process. Instead, it learns the boundary for the normal data points and identifies the data outside the border to be anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bb51d",
   "metadata": {},
   "source": [
    "**31. How do you choose the appropriate threshold for anomaly detection?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29f405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90323973",
   "metadata": {},
   "source": [
    "**32. How do you handle imbalanced datasets in anomaly detection?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b116aaa3",
   "metadata": {},
   "source": [
    "The various techniques you can use to handle imbalanced datasets.\n",
    "1.Random Undersampling and Oversampling. Source.\n",
    "2.Undersampling and Oversampling using imbalanced-learn. \n",
    "3.Class weights in the models. \n",
    "4.Change your Evaluation Metric. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741581f",
   "metadata": {},
   "source": [
    "**33. Give an example scenario where anomaly detection can be applied.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1bb3422",
   "metadata": {},
   "source": [
    "One of the clearest anomaly detection examples is for preventing fraud. For example, a credit card company will use anomaly detection to track how customers typically use their credit cards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68dd1c",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89febef3",
   "metadata": {},
   "source": [
    "**34. What is dimension reduction in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bb1c228",
   "metadata": {},
   "source": [
    "Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df2391",
   "metadata": {},
   "source": [
    "**35. Explain the difference between feature selection and feature extraction.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94916037",
   "metadata": {},
   "source": [
    "Feature selection is a process of selecting a subset of relevant features from the original set of features. The goal is to reduce the dimensionality of the feature space, simplify the model, and improve its generalization performance.\n",
    "Feature extraction is a process of transforming the original features into a new set of features that are more informative and compact. The goal is to capture the essential information from the original features and represent it in a lower-dimensional feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89213894",
   "metadata": {},
   "source": [
    "**36. How does Principal Component Analysis (PCA) work for dimension reduction?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c87b1d27",
   "metadata": {},
   "source": [
    "Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e338a",
   "metadata": {},
   "source": [
    "**37. How do you choose the number of components in PCA?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea83bc",
   "metadata": {},
   "source": [
    "The choice of k is done by selection of the smallest values of k, which has a variance ratio higher than a specific threshold, 99% for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d9d00",
   "metadata": {},
   "source": [
    "**38. What are some other dimension reduction techniques besides PCA?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f8d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some other dimension reduction techniques besides PCA:\n",
    "1. Feature selection. \n",
    "2. Feature extraction. \n",
    "3. Linear discriminant analysis (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fed7fa",
   "metadata": {},
   "source": [
    "**39. Give an example scenario where dimension reduction can be applied.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa703fab",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e69e8",
   "metadata": {},
   "source": [
    "**40. What is feature selection in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad56b44f",
   "metadata": {},
   "source": [
    "Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7e1d3",
   "metadata": {},
   "source": [
    "**41. Explain the difference between filter, wrapper, and embedded methods of feature selection.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35b1f3e0",
   "metadata": {},
   "source": [
    "Filter methods perform the feature selection independently of construction of the classification model. \n",
    "Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. \n",
    "In embedded methods the feature selection is an integral part of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c898559",
   "metadata": {},
   "source": [
    "**42. How does correlation-based feature selection work?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24b87112",
   "metadata": {},
   "source": [
    "eatures with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7dd16e",
   "metadata": {},
   "source": [
    "**43. How do you handle multicollinearity in feature selection?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c8a7354",
   "metadata": {},
   "source": [
    "Removing variables. A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation.\n",
    "Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA). ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3eabb1",
   "metadata": {},
   "source": [
    "**44. What are some common feature selection metrics?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some common feature selection metrics are : \n",
    "1.Chi-square Test.\n",
    "2.Correlation Coefficient.\n",
    "3.Dispersion Ratio.\n",
    "4.Backward Feature Elimination.\n",
    "5.Recursive Feature Elimination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907e5cb",
   "metadata": {},
   "source": [
    "**45. Give an example scenario where feature selection can be applied.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Below are some real-life example of feature selection:\n",
    "Criminal behavior modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb57b9",
   "metadata": {},
   "source": [
    "# Data Drift Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a2a97",
   "metadata": {},
   "source": [
    "**46. What is data drift in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d52f27aa",
   "metadata": {},
   "source": [
    "Data drift is the change in model input data that leads to model performance degradation. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd5d48",
   "metadata": {},
   "source": [
    "**47. Why is data drift detection important?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5a1eebc",
   "metadata": {},
   "source": [
    "Data drift is the change in model input data that leads to model performance degradation. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use. Monitoring data drift helps detect these model performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561fe4d",
   "metadata": {},
   "source": [
    "**48. Explain the difference between concept drift and feature drift.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1b904c2",
   "metadata": {},
   "source": [
    "Concept drift refers to a changing underlying goal or objective for the model. Both data drift and concept drift(feature drift) can lead to a decline in the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bb0cf",
   "metadata": {},
   "source": [
    "**49. What are some techniques used for detecting data drift?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0f568f4",
   "metadata": {},
   "source": [
    "Some of the popular statistical methods for detecting data drift are Population Stability Index, Kullback-Leiber or KL Divergence, Jenson-Shannon or JS Divergence, Kolmogorov-Smirnov Test or KS Test, Wasserstein Metric or Earth Mover Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a91c77",
   "metadata": {},
   "source": [
    "**50. How can you handle data drift in a machine learning model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db720291",
   "metadata": {},
   "outputs": [],
   "source": [
    "The generic way to monitor data drift are :\n",
    "\n",
    "1.First, the training data set is collected and curated, \n",
    "2.then the model is trained on that. \n",
    "3.the model is continuously monitored against a golden data set which is curated by human experts. \n",
    "4.If the performance score is degraded below a threshold, an alarm is triggered to re-train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd51797",
   "metadata": {},
   "source": [
    "# Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242bc47",
   "metadata": {},
   "source": [
    "**51. What is data leakage in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99be03f0",
   "metadata": {},
   "source": [
    "Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.\n",
    "In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe82727",
   "metadata": {},
   "source": [
    "**52. Why is data leakage a concern?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5bb682d",
   "metadata": {},
   "source": [
    "Data leakage is one of the major problems in machine learning which occurs when the data that we are using to train an ML algorithm has the information the model is trying to predict. It is a situation that causes unpredictable and bad prediction outcomes after model deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bc701",
   "metadata": {},
   "source": [
    "**53. Explain the difference between target leakage and train-test contamination.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b9764",
   "metadata": {},
   "source": [
    "**Target leakage :**\n",
    "Target leakage occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the timing or chronological order that data becomes available, not merely whether a feature helps make good predictions.\n",
    "\n",
    "**train-test contamination:**\n",
    "Recall that validation is meant to be a measure of how the model does on data that it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavior. This is sometimes called train-test contamination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec845c",
   "metadata": {},
   "source": [
    "**54. How can you identify and prevent data leakage in a machine learning pipeline?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25b44022",
   "metadata": {},
   "source": [
    "Some techniques by which you can identify and prevent data leakage in Machine learning:\n",
    "    \n",
    "1.Extract the appropriate set of features\n",
    "2.Add an individual validation set\n",
    "3.Apply data pre-processing separately to both data sets\n",
    "4.Time-series data\n",
    "5.Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851622b",
   "metadata": {},
   "source": [
    "**55. What are some common sources of data leakage?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69905cdc",
   "metadata": {},
   "source": [
    "Most Common Causes of Data leakage are : \n",
    "1. Weak and Stolen Credentials, a.k.a. Passwords.\n",
    "2. Back Doors, Application Vulnerabilities.\n",
    "3. Malware.\n",
    "4. Too Many Permissions.\n",
    "5. Physical Attacks.\n",
    "6. Improper Configuration, User Error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74de64b",
   "metadata": {},
   "source": [
    "**56. Give an example scenario where data leakage can occur.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbd0114e",
   "metadata": {},
   "source": [
    "Imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data look like this:\n",
    "\n",
    "got_pneumonia\tage\tweight\tmale\ttook_antibiotic_medicine\t...\n",
    "        False\t65\t100\tFalse\tFalse\t...\n",
    "        False\t72\t130\tTrue\tFalse\t...\n",
    "        True\t58\t100\tFalse\tTrue\t...\n",
    "People take antibiotic medicines after getting pneumonia in order to recover. The raw data shows a strong relationship between those columns, but took_antibiotic_medicine is frequently changed after the value for got_pneumonia is determined. This is target leakage.\n",
    "\n",
    "The model would see that anyone who has a value of False for took_antibiotic_medicine didn't have pneumonia. Since validation data comes from the same source as training data, the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores.\n",
    "\n",
    "But the model will be very inaccurate when subsequently deployed in the real world, because even patients who will get pneumonia won't have received antibiotics yet when we need to make predictions about their future health.\n",
    "\n",
    "To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d53c5",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602452c4",
   "metadata": {},
   "source": [
    "**57. What is cross-validation in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a5976e6",
   "metadata": {},
   "source": [
    "Cross-Validation is a statistical method of evaluating and comparing learning algorithms by dividing data into two segments: one used to learn or train a model and the other used to validate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b17a0",
   "metadata": {},
   "source": [
    "**58. Why is cross-validation important?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a55285f9",
   "metadata": {},
   "source": [
    "The main advantage of cross-validation is that it provides an estimate of the performance of the model on new data, which is important for assessing the model's generalizability. It also helps to avoid overfitting,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6a1a6",
   "metadata": {},
   "source": [
    "**59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "212ff99d",
   "metadata": {},
   "source": [
    "Difference between k-fold cross-validation and stratified k-fold cross-validation\n",
    "1.When we are dealing with classification problem of imbalance class distribution, we have to use StratifiedKFold.\n",
    "2.KFold devides the dataset into k folds.\n",
    "3.Where as Stratified ensures that each fold of dataset has the same proportion of observations with a given label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dfe33c",
   "metadata": {},
   "source": [
    "**60. How do you interpret the cross-validation results?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b012156",
   "metadata": {},
   "source": [
    "K-fold Cross-Validation is when the dataset is split into a K number of folds and is used to evaluate the model's ability when given new data. K refers to the number of groups the data sample is split into. For example, if you see that the k-value is 5, we can call this a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816f4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
