{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e492c457",
   "metadata": {},
   "source": [
    "# GENERAL LINEAR MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75117849",
   "metadata": {},
   "source": [
    "**1. What is the purpose of the General Linear Model (GLM)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10837c47",
   "metadata": {},
   "source": [
    "The General Linear Model (GLM) underlies most of the statistical analyses that are used in applied and social research. It is the foundation for the t-test, Analysis of Variance (ANOVA), Analysis of Covariance (ANCOVA), regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91429ecd",
   "metadata": {},
   "source": [
    "**2. What are the key assumptions of the General Linear Model?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65f70c5d",
   "metadata": {},
   "source": [
    "The key assumptions of the General Linear Model are linearity, homoskedasticity (constant variance), normality, and independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5d816",
   "metadata": {},
   "source": [
    "**3. How do you interpret the coefficients in a GLM?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea8e8b17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0c7fdab",
   "metadata": {},
   "source": [
    "**4. What is the difference between a univariate and multivariate GLM?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47214419",
   "metadata": {},
   "source": [
    "Univariate analysis is the analysis of one variable. Multivariate analysis is the analysis of more than one variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458814cf",
   "metadata": {},
   "source": [
    "**5. Explain the concept of interaction effects in a GLM.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c963e1a1",
   "metadata": {},
   "source": [
    "Interactions are tested using product term coefficients, which are then interpreted as the degree to which the effect of a focal predictor on the outcome changes for every unit change in the other variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6f768",
   "metadata": {},
   "source": [
    "**6. How do you handle categorical predictors in a GLM?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164bd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e750fd13",
   "metadata": {},
   "source": [
    "**7. What is the purpose of the design matrix in a GLM?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110672d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d298b732",
   "metadata": {},
   "source": [
    "**8. How do you test the significance of predictors in a GLM?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e65b378",
   "metadata": {},
   "source": [
    "If the p-value is less than or equal to the significance level, you can conclude that there is a statistically significant association between the response variable and the term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21374fde",
   "metadata": {},
   "source": [
    "**9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bad7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a478ef",
   "metadata": {},
   "source": [
    "**10. Explain the concept of deviance in a GLM.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50492f6c",
   "metadata": {},
   "source": [
    "Deviance is a measure of error; lower deviance means better fit to data. The greater the deviance, the worse the model fits compared to the best case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd1167",
   "metadata": {},
   "source": [
    "# REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41ab76",
   "metadata": {},
   "source": [
    "**11. What is regression analysis and what is its purpose?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "289fa197",
   "metadata": {},
   "source": [
    "Regression analysis is a statistical method that shows the relationship between two or more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd6fd8",
   "metadata": {},
   "source": [
    "**12. What is the difference between simple linear regression and multiple linear regression?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a31ae536",
   "metadata": {},
   "source": [
    "linear regress only has one independent variable impacting the slope of the relationship, multiple regression incorporates multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835acfa",
   "metadata": {},
   "source": [
    "**13. How do you interpret the R-squared value in regression?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fe99bf7",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure that indicates how much of the variation of a dependent variable is explained by an independent variable in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71013c72",
   "metadata": {},
   "source": [
    "**14. What is the difference between correlation and regression?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d7be95f",
   "metadata": {},
   "source": [
    "Correlation can be either negative or positive. If the two variables move in the same direction, i.e. an increase in one variable results in the corresponding increase in another variable, and vice versa, then the variables are considered to be positively correlated.\n",
    "A statistical technique based on the average mathematical relationship between two or more variables is known as regression, to estimate the change in the metric dependent variable due to the change in one or more independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aead1fa",
   "metadata": {},
   "source": [
    "**15. What is the difference between the coefficients and the intercept in regression?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0b62299",
   "metadata": {},
   "source": [
    "The simple linear regression model is essentially a linear equation of the form y = c + b*x; where y is the dependent variable (outcome), x is the independent variable (predictor), b is the slope of the line; also known as regression coefficient and c is the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06b8fa",
   "metadata": {},
   "source": [
    "**16. How do you handle outliers in regression analysis?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bdab637",
   "metadata": {},
   "source": [
    "1. Delete them.\n",
    "2. The use of mode and median-based methods.\n",
    "3. The use of robust methods, in which such values may be included in our\n",
    "calculations, but given less weight, i.e. importance, in plotting the\n",
    "regression line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8fe4",
   "metadata": {},
   "source": [
    "**17. What is the difference between ridge regression and ordinary least squares regression?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cab11e0",
   "metadata": {},
   "source": [
    "The regularisation parameter (lambda) in ridge regression aids in managing the trade-off between minimizing the magnitude of the coefficients and minimising the residual sum of squares. Ridge regression can lessen the variance in the model by adding a penalty term, which lessens overfitting and improves generalisation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55768a2f",
   "metadata": {},
   "source": [
    "**18. What is heteroscedasticity in regression and how does it affect the model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e9ecf9",
   "metadata": {},
   "source": [
    "Heteroskedasticity refers to situations where the variance of the residuals is unequal over a range of measured values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433d810",
   "metadata": {},
   "source": [
    "**19. How do you handle multicollinearity in regression analysis?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2489e846",
   "metadata": {},
   "source": [
    "1.Remove some of the highly correlated independent variables.\n",
    "2.Linearly combine the independent variables, such as adding them together.\n",
    "3.Partial least squares regression uses principal component analysis to create a set of uncorrelated components to include in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39dda8",
   "metadata": {},
   "source": [
    "**20. What is polynomial regression and when is it used?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6c92cfb",
   "metadata": {},
   "source": [
    "A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line and Polynomial regression is used when there is no linear correlation between the variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cfbefe",
   "metadata": {},
   "source": [
    "# LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cda2ae",
   "metadata": {},
   "source": [
    "**21. What is a loss function and what is its purpose in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71e1f5e5",
   "metadata": {},
   "source": [
    "A loss function is a mathematical function that quantifies the difference between predicted and actual values in a machine learning model. It measures the model's performance and guides the optimization process by providing feedback on how well it fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe6578",
   "metadata": {},
   "source": [
    "**22. What is the difference between a convex and non-convex loss function?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6152fd19",
   "metadata": {},
   "source": [
    "Convex loss function problems have only one local optimum point, which is also the global minima point. On the other hand, non convex loss functions problems have multiple global minima points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e5f3a",
   "metadata": {},
   "source": [
    "**23. What is mean squared error (MSE) and how is it calculated?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abf771",
   "metadata": {},
   "source": [
    " Mean square error (MSE) is the average of the square of the error and Mean square error is calculated by taking the average, specifically the mean, of errors squared from data as it relates to a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bf756",
   "metadata": {},
   "source": [
    "**24. What is mean absolute error (MAE) and how is it calculated?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9455b85",
   "metadata": {},
   "source": [
    "The mean absolute error (MAE) is defined as the average variance between the significant values in the dataset and the projected values in the same dataset.\n",
    "Mean Absolute Error (MAE) is calculated by taking the summation of the absolute difference between the actual and calculated values of each observation over the entire array and then dividing the sum obtained by the number of observations in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbefcd5",
   "metadata": {},
   "source": [
    "**25. What is log loss (cross-entropy loss) and how is it calculated?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6fc2406",
   "metadata": {},
   "source": [
    "Log-loss measures the performance of a model by quantifying the difference between predicted probabilities and actual values.\n",
    "Log-loss value is calculated for each observation based on observation's actual value (y) and prediction probability (p). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d346f",
   "metadata": {},
   "source": [
    "**26. How do you choose the appropriate loss function for a given problem?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704b822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac9d4a58",
   "metadata": {},
   "source": [
    "**27. Explain the concept of regularization in the context of loss functions.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc8ff17c",
   "metadata": {},
   "source": [
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777f099",
   "metadata": {},
   "source": [
    "**28. What is Huber loss and how does it handle outliers?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46e9355b",
   "metadata": {},
   "source": [
    "Huber loss functions are less sensitive to outliers than Mean Squared Error, giving a more accurate prediction or estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb229f",
   "metadata": {},
   "source": [
    "**29. What is quantile loss and when is it used?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c181c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The quantile regression loss function is applied to predict quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1610bd",
   "metadata": {},
   "source": [
    "**30. What is the difference between squared loss and absolute loss?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7350ac",
   "metadata": {},
   "source": [
    "# OPTIMIZER (GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc06c7",
   "metadata": {},
   "source": [
    "**31. What is an optimizer and what is its purpose in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9df7c031",
   "metadata": {},
   "source": [
    "Optimizers are algorithms or methods used to minimize an error function(loss function)or to maximize the efficiency of production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da3dcb",
   "metadata": {},
   "source": [
    "**32. What is Gradient Descent (GD) and how does it work?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d671141",
   "metadata": {},
   "source": [
    "Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function\n",
    "It works by iteratively adjusting the weights or parameters of the model in the direction of the negative gradient of the cost function until the minimum of the cost function is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a945223",
   "metadata": {},
   "source": [
    "**33. What are the different variations of Gradient Descent?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "some popular Gradient descent variants:\n",
    "1. Batch Gradient Descent\n",
    "2. Stochastic Gradient Descent (SGD)\n",
    "3. Mini-batch Gradient Descent\n",
    "4. Adagrad\n",
    "5. Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c4393",
   "metadata": {},
   "source": [
    "**34. What is the learning rate in GD and how do you choose an appropriate value?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcbf426f",
   "metadata": {},
   "source": [
    "Learning rate in GD determines how fast or slow we will move towards the optimal weights. If the learning rate is very large we will skip the optimal solution.\n",
    "For the gradient descent algorithm to reach the local minimum we must set the learning rate to an appropriate value, which is neither too low nor too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d59a8e",
   "metadata": {},
   "source": [
    "**35. How does GD handle local optima in optimization problems?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1700e450",
   "metadata": {},
   "source": [
    "Gradient descent is driven by the gradient, which will be zero at the base of any minima. Local minimum are called so since the value of the loss function is minimum at that point in a local region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce1270",
   "metadata": {},
   "source": [
    "**36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1921fa49",
   "metadata": {},
   "source": [
    "In Gradient Descent, we consider all the points in calculating loss and derivative, while in Stochastic gradient descent, we use single point in loss function and its derivative randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde05ef2",
   "metadata": {},
   "source": [
    "**37. Explain the concept of batch size in GD and its impact on training.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4a24c34",
   "metadata": {},
   "source": [
    "Batch size is important because it affects both the training time and the generalization of the model. A smaller batch size allows the model to learn from each individual example but takes longer to train. A larger batch size trains faster but may result in the model not capturing the nuances in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090a190",
   "metadata": {},
   "source": [
    "**38. What is the role of momentum in optimization algorithms?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "169410c2",
   "metadata": {},
   "source": [
    "Momentum factor assists the optimizer in continuing to go in the same direction even if the gradient changes direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14513d1",
   "metadata": {},
   "source": [
    "**39. What is the difference between batch GD, mini-batch GD, and SGD?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "913c31c9",
   "metadata": {},
   "source": [
    "Batch GD : In Batch Gradient Descent, all the training data is taken into consideration to take a single step. \n",
    "\n",
    "Mini-batch GD : when we are using the mini-batch gradient descent we are updating our parameters frequently as well as we can use vectorized implementation for faster computations.\n",
    "\n",
    "Stochastic gradient descen: In Stochastic gradient descent, we use single point in loss function and its derivative randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e1fd0",
   "metadata": {},
   "source": [
    "**40. How does the learning rate affect the convergence of GD?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f9f76b9",
   "metadata": {},
   "source": [
    "If the learning rate is very large we will skip the optimal solution. If it is too small we will need too many iterations to converge to the best values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e29bd9",
   "metadata": {},
   "source": [
    "# REGULARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23f069",
   "metadata": {},
   "source": [
    "**41. What is regularization and why is it used in machine learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50de2657",
   "metadata": {},
   "source": [
    "Regularization is a technique to prevent the model from overfitting by adding extra information to it. Sometimes the machine learning model performs well with the training data but does not perform well with the test data.\n",
    "Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d7939",
   "metadata": {},
   "source": [
    "**42. What is the difference between L1 and L2 regularization?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "081c2c55",
   "metadata": {},
   "source": [
    "L1 Regularization, also called a lasso regression, adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function. L2 Regularization, also called a ridge regression, adds the “squared magnitude” of the coefficient as the penalty term to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a59546",
   "metadata": {},
   "source": [
    "**43. Explain the concept of ridge regression and its role in regularization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5836d",
   "metadata": {},
   "source": [
    "Ridge regression also known as L2 regularization is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions. This model is a model tuning method that is used to analyse any data that suffers from multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70295702",
   "metadata": {},
   "source": [
    "**44. What is the elastic net regularization and how does it combine L1 and L2 penalties?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a62e97e",
   "metadata": {},
   "source": [
    "Elastic Net Regularization is a regularization technique that uses both L1 and L2 regularizations to produce most optimized output.Elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge method to get the most optimized result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e515924",
   "metadata": {},
   "source": [
    "**45. How does regularization help prevent overfitting in machine learning models?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dabaf2ae",
   "metadata": {},
   "source": [
    "Regularization help prevent overfitting in machine learning modelswhere the model has to choose what features to give weight to wisely, and to reduce or eliminate the weight on less important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda3444",
   "metadata": {},
   "source": [
    "**46. What is early stopping and how does it relate to regularization?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82608f31",
   "metadata": {},
   "source": [
    "Early stopping is an optimization technique used to reduce overfitting without compromising on model accuracy like the way regularization techniques works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e353c2",
   "metadata": {},
   "source": [
    "**47. Explain the concept of dropout regularization in neural networks.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e886222",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e750e47",
   "metadata": {},
   "source": [
    "**48. How do you choose the regularization parameter in a model?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccc2cafb",
   "metadata": {},
   "source": [
    "The regularization parameter which gives the lowest MSE on the validation set in a model we will choose that regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81531c74",
   "metadata": {},
   "source": [
    "**49. What is the difference between feature selection and regularization?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "679c5c43",
   "metadata": {},
   "source": [
    "Feature selection is a variable selection, or attribute selection. This approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model inference whereas Regularization is a technique to prevent the model from overfitting by adding extra information to it. Sometimes the machine learning model performs well with the training data but does not perform well with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f43f6b",
   "metadata": {},
   "source": [
    "**50. What is the trade-off between bias and variance in regularized models?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "134be1c8",
   "metadata": {},
   "source": [
    "Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff. An ideal algorithm should neither underfit nor overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2a0b3",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINE (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a5eef3",
   "metadata": {},
   "source": [
    "**51. What is Support Vector Machines (SVM) and how does it work?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4959c8e",
   "metadata": {},
   "source": [
    "SVM or Support Vector Machine is a linear model for classification and regression problems.\n",
    "The algorithm creates a line or a hyperplane which separates the data into classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6c793",
   "metadata": {},
   "source": [
    "**52. How does the kernel trick work in SVM?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65bdac1c",
   "metadata": {},
   "source": [
    "Kernel trick accepts inputs in the original lower dimensional space and returns the dot product of the transformed vectors in the higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3154d80",
   "metadata": {},
   "source": [
    "**53. What are support vectors in SVM and why are they important?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "392e49d9",
   "metadata": {},
   "source": [
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5ea64",
   "metadata": {},
   "source": [
    "**54. Explain the concept of the margin in SVM and its impact on model performance.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f229ac5",
   "metadata": {},
   "source": [
    "Margin: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa0f8c",
   "metadata": {},
   "source": [
    "**55. How do you handle unbalanced datasets in SVM?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3a960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e0b5d73",
   "metadata": {},
   "source": [
    "**56. What is the difference between linear SVM and non-linear SVM?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb963d38",
   "metadata": {},
   "source": [
    "When we can easily separate data with hyperplane by drawing a straight line is Linear SVM. When we cannot separate data with a straight line we use Non – Linear SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca7756",
   "metadata": {},
   "source": [
    "**57. What is the role of C-parameter in SVM and how does it affect the decision boundary?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a07630bf",
   "metadata": {},
   "source": [
    "C Parameter is used for controlling the outliers — low C implies we are allowing more outliers, high C implies we are allowing fewer outliers. High C (cost) means the cost of misclassification is increased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff2a4a",
   "metadata": {},
   "source": [
    "**58. Explain the concept of slack variables in SVM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c476fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5496e34",
   "metadata": {},
   "source": [
    "**59. What is the difference between hard margin and soft margin in SVM?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b40b195",
   "metadata": {},
   "source": [
    "Margin: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin. \n",
    "The difference between a hard margin and a soft margin in SVMs lies in the separability of the data. If our data is linearly separable, we go for a hard margin. However, if this is not the case, it won’t be feasible to do that. In the presence of the data points that make it impossible to find a linear classifier, we would have to be more lenient and let some of the data points be misclassified. In this case, a soft margin SVM is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3caf8",
   "metadata": {},
   "source": [
    "**60. How do you interpret the coefficients in an SVM model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f8655",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24241ec3",
   "metadata": {},
   "source": [
    "**61. What is a decision tree and how does it work?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67656e80",
   "metadata": {},
   "source": [
    "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks.\n",
    "It follows a tree-like model of decisions and their possible consequences. The algorithm works by recursively splitting the data into subsets based on the most significant feature at each node of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9ed64",
   "metadata": {},
   "source": [
    "**62. How do you make splits in a decision tree?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e2aeba6",
   "metadata": {},
   "source": [
    "1.For each split, calculate the entropy of each child node independently.\n",
    "2.Calculate the entropy of each split using the weighted average entropy of child nodes.\n",
    "3.Choose the split with the lowest entropy or the greatest gain in information.\n",
    "4.Repeat these steps to obtain homogeneous split nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b6527",
   "metadata": {},
   "source": [
    "**63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd5b249e",
   "metadata": {},
   "source": [
    "Impurity measures are used in Decision Trees just like squared loss function in linear regression. We try to arrive at as lowest impurity as possible by the algorithm of our choice. \n",
    "Calculate the entropy of each split using the weighted average entropy of child nodes till obtaining the pure split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae621c8",
   "metadata": {},
   "source": [
    "**64. Explain the concept of information gain in decision trees.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72675306",
   "metadata": {},
   "source": [
    "Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb63794",
   "metadata": {},
   "source": [
    "**65. How do you handle missing values in decision trees?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ecdc96b",
   "metadata": {},
   "source": [
    "Decision Trees handle missing values in the following ways: Fill the missing attribute value with the most common value of that attribute. Fill in the missing value by assigning a probability to each of the possible values of the attribute based on other samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f257056",
   "metadata": {},
   "source": [
    "**66. What is pruning in decision trees and why is it important?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce0296b2",
   "metadata": {},
   "source": [
    "Pruning is a technique that removes the parts of the Decision Tree which prevent it from growing to its full depth. The parts that it removes from the tree are the parts that do not provide the power to classify instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b930527",
   "metadata": {},
   "source": [
    "**67. What is the difference between a classification tree and a regression tree?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b799a7ef",
   "metadata": {},
   "source": [
    "Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067ed21",
   "metadata": {},
   "source": [
    "**68. How do you interpret the decision boundaries in a decision tree?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424fcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6e86b1",
   "metadata": {},
   "source": [
    "**69. What is the role of feature importance in decision trees?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision Tree is feature importances that helps us understand which features are actually helpful compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc6456e",
   "metadata": {},
   "source": [
    "**70. What are ensemble techniques and how are they related to decision trees?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d7758a4",
   "metadata": {},
   "source": [
    "Ensemble methods, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904facba",
   "metadata": {},
   "source": [
    "# ENSEMBLE TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf5853",
   "metadata": {},
   "source": [
    "**71. What are ensemble techniques in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ee42a",
   "metadata": {},
   "source": [
    "Ensemble methods, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e574454",
   "metadata": {},
   "source": [
    "**72. What is bagging and how is it used in ensemble learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da6a9a2c",
   "metadata": {},
   "source": [
    "Bagging is a powerful ensemble method which helps to reduce variance, and by extension, prevent overfitting.Ensemble methods improve model precision by using a group (or \"ensemble\") of models which, when combined, outperform individual models when used separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d179f424",
   "metadata": {},
   "source": [
    "**73. Explain the concept of bootstrapping in bagging.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c527bedf",
   "metadata": {},
   "source": [
    "Bootstrapping is the process of generating bootstrapped samples from the given dataset. The samples are formulated by randomly drawing the data points with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16725381",
   "metadata": {},
   "source": [
    "**74. What is boosting and how does it work?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00b1f7b1",
   "metadata": {},
   "source": [
    "Boosting methods are focused on iteratively combining weak learners to build a strong learner that can predict more accurate outcomes.In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d451176",
   "metadata": {},
   "source": [
    "**75. What is the difference between AdaBoost and Gradient Boosting?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83e06b99",
   "metadata": {},
   "source": [
    "Adaboost is computed with a specific loss function and becomes more rigid when comes to few iterations. But in gradient boosting, it assists in finding the proper solution to additional iteration modeling problem as it is built with some generic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b7fc7",
   "metadata": {},
   "source": [
    "**76. What is the purpose of random forests in ensemble learning?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "364c84e4",
   "metadata": {},
   "source": [
    "The random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method that is better than a single decision tree because it reduces the over-fitting by averaging the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dcb3ed",
   "metadata": {},
   "source": [
    "**77. How do random forests handle feature importance?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb3c8f03",
   "metadata": {},
   "source": [
    "Each tree of the random forest can calculate the importance of a feature according to its ability to increase the pureness of the leaves. It's a topic related to how Classification And Regression Trees (CART) work. The higher the increment in leaves purity, the higher the importance of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7babb43a",
   "metadata": {},
   "source": [
    "**78. What is stacking in ensemble learning and how does it work?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c871890",
   "metadata": {},
   "source": [
    "Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. \n",
    "Stacking enables us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250d341",
   "metadata": {},
   "source": [
    "**79. What are the advantages and disadvantages of ensemble techniques?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0b7510c",
   "metadata": {},
   "source": [
    "Ensemble methods offer several advantages over single models :\n",
    "It improves accuracy and performance, especially for complex and noisy problems. \n",
    "They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance.\n",
    "\n",
    "Ensemble methods offer several disadvantages over single models :\n",
    "Ensembling is less interpretable, the output of the ensembled model is hard to predict and explain.\n",
    "The art of ensembling is hard to learn and any wrong selection can lead to lower predictive accuracy than an individual model.\n",
    "Ensembling is expensive in terms of both time and space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d8761",
   "metadata": {},
   "source": [
    "**80. How do you choose the optimal number of models in an ensemble?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50848b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
